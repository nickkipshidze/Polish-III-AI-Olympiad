{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multispectral Data Segmentation\n",
    "\n",
    "![satellite](https://hackmd.io/_uploads/r1lA5vkx-l.jpg)\n",
    "\n",
    "*Image generated by the DeepAI image generation tool.*\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The Earth is constantly changing—cities are expanding, forest boundaries are shifting, and glaciers are melting. To understand these processes, scientists and engineers are increasingly turning to satellite data. Thanks to it, we can observe our planet from space, monitor the state of the environment, and detect changes that are often invisible to the \"naked eye.\"\n",
    "\n",
    "One of the key types of such data is multispectral imagery, which are photos taken in many different ranges of the electromagnetic spectrum. Each band provides different information about the Earth's surface, specifically about how a given material absorbs and reflects light. For example: in visible light, we see the natural colors of plants and soil, while infrared allows us to assess the condition of vegetation or soil moisture.\n",
    "\n",
    "This technology enables the non-invasive creation of maps, analysis of agricultural land (e.g., for fertility), tracking the progress of droughts, as well as monitoring pollution or the effects of natural disasters. Today, multispectral images are one of the cornerstones of modern remote sensing.\n",
    "\n",
    "Every material on Earth—water, sand, concrete, or grass—reflects radiation in its own characteristic way. We call this unique pattern a spectral signature. By analyzing these signatures in different bands, we can precisely identify the objects visible in the image.\n",
    "\n",
    "In practice, however, this data can be distorted by atmospheric factors (clouds, water vapor, dust) and minor sensor measurement errors. Therefore, before multispectral images can be used for analysis, they must undergo appropriate processing and correction.\n",
    "\n",
    "## Task\n",
    "\n",
    "Your task is to prepare a class for processing the dataset and to train a model for segmenting multispectral images (terrain maps).\n",
    "\n",
    "The first stage of the task involves creating a function that will process the input data in a way that maximizes the model's effectiveness (without using labels). To increase the diversity of the training set and improve generalization ability, you can use various data augmentation techniques, such as rotation, scaling, or adding noise (or other methods you deem appropriate).\n",
    "\n",
    "During data analysis, try to select a **minimum** set of bands (channels) that will allow you to achieve high-quality segmentation. Consider which channels carry the most information—for example: does infrared affect the detection of vegetation? Using all bands is not always necessary; often, better results are achieved by selecting only those that contain key information for distinguishing between classes. Remember, you do not have to limit yourself to raw channel values—in the processing stage, you can generate entirely new feature representations.\n",
    "\n",
    "The second stage is to create a solution for the segmentation of multispectral images, which means assigning one of four classes to each pixel: water, land, vegetation, or industrial areas (as in the example below). For this purpose, you can use libraries such as PyTorch and scikit-learn.\n",
    "\n",
    "![segmentation_maps](https://live.staticflickr.com/65535/54927654414_b325b31a02_c.jpg)\n",
    "\n",
    "You have at your disposal a labeled training and validation set on which you can test your approach. The final evaluation of the model will be carried out on a hidden test set. Each input image consists of 12 spectral bands, but the decision on how many and which of them you will use is up to you.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The data has been divided into separate sets:\n",
    "\n",
    "- **training** - $48$ samples (images),\n",
    "\n",
    "- **validation** - $16$ samples (images).\n",
    "\n",
    "The final evaluation will use a **test** set consisting of $96$ samples (images), to which you do not have access.\n",
    "Each sample is an image with dimensions of $30 \\times 30$ pixels. Each pixel has an assigned class label (segmentation mask).\n",
    "Each pixel is described by $12$ channels corresponding to measurements of light reflectance in different wavelength ranges.\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your score depends on two factors: the quality of segmentation on the hidden test set and the number of channels you decide to use at the model's input.\n",
    "\n",
    "The first component of the evaluation function is related to the number of channels used, $N\\_channels$. Originally, each pixel is composed of 12 channels, and for using all channels, you will receive 0 points for this part of the task. All solutions using 3 or fewer channels will receive full points for this component of the evaluation function, while solutions using $\\lbrace 4, 5, 6, ..., 11\\rbrace$ channels will be evaluated according to a quadratic scaling function:\n",
    "\n",
    "$$\\mathtt{channel\\_evaluation} = \n",
    "\\begin{cases} \n",
    "    0 &\\quad \\text{if }  N\\_channels \\geq 12 \\\\\n",
    "    100 &\\quad \\text{if }  N\\_channels \\leq 3 \\\\\n",
    "    100 \\cdot \\left( \\dfrac{12 - N\\_channels}{12 - 3} \\right)^2 &\\quad \\text{otherwise}.\n",
    "\\end{cases}$$\n",
    "\n",
    "For example, for using 10 channels, you will receive only 5 points for this part of the task, multiplied by a factor of $0.25$.\n",
    "The second component of the evaluation function is related to the quality of segmentation for the maps used in the task. We will use the $\\text{IoU}$ (Intersection over Union) metric, which evaluates the ratio of correctly classified pixels of a given class in a given image to the total number of pixels of that class present in the ground truth map or the model's prediction. Finally, an average will be calculated over all classes and images in the test set. For each image, the average will be calculated only over the classes that appear in the *ground truth*.\n",
    "\n",
    "$$\\text{IoU} = \\dfrac{1}{M \\cdot N} \\cdot \\sum\\limits_{i=1}^{N} \\sum\\limits_{j=1}^{M_i} \\dfrac{\\text{number of correctly classified pixels of the j-th class in the i-th map}}{\\text{number of pixels of the j-th class in the combined ground truth and model prediction area for the i-th map}}$$\n",
    "\n",
    "where $N$ is the number of images in a given set (e.g., test set), and $M_i$ is the number of classes actually present in the $i$-th map. If the average $\\text{IoU}$ is no more than 0.6, you will receive 0 points for this part of the task, and if it is at least 0.8, you will receive the full score for this part.\n",
    "\n",
    "$$\\mathtt{segmentation\\_evaluation} = \n",
    "\\begin{cases} \n",
    "    0 &\\quad \\text{if }  \\text{IoU} \\leq 0.6 \\\\\n",
    "    100 &\\quad \\text{if }  \\text{IoU} \\geq 0.8 \\\\\n",
    "    100 \\cdot \\dfrac{\\text{IoU} - 0.6}{0.8 - 0.6} &\\quad \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "An example of calculating $\\text{IoU}$ for one of the classes (represented by blue squares) is shown in the image below. The final $\\text{IoU}$ score is averaged over all classes present in a given image, as well as over all images in the set.\n",
    "\n",
    "![IoU](https://live.staticflickr.com/65535/54922482577_1a5222581f_b.jpg)\n",
    "\n",
    "The final score for the task will be $25\\%$ from the evaluation for the number of channels used and $75\\%$ from the points for segmentation quality, according to the formula:\n",
    "\n",
    "$$\\text{score} = 0.25 \\cdot \\mathtt{channel\\_evaluation} + 0.75 \\cdot \\mathtt{segmentation\\_evaluation}$$\n",
    "\n",
    "**NOTE!** If the $\\text{IoU}$ is less than 0.5, you will receive 0 points for the entire task, regardless of the number of channels used!\n",
    "\n",
    "\n",
    "## Constraints\n",
    "\n",
    "- Your solution will be tested on the Competition Platform without internet access and in an environment with a GPU.\n",
    "- Model training and the evaluation of your final solution on the Competition Platform cannot take longer than 5 minutes using a GPU.\n",
    "- The class performing the initial data transformation, `YourPreprocessing`, cannot use the dataset labels in any way (it can only process the samples themselves), and the segmentation must be performed directly in the `YourModel` class.\n",
    "\n",
    "## Submission Files\n",
    "\n",
    "This notebook completed with your solution (see the `YourModel` class and the `YourPreprocessing` class), in which you will prepare a setup that includes data transformation with potential channel reduction and modification, and a model that performs segmentation on the processed data.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Remember that during the check, the `FINAL_EVALUATION_MODE` flag will be set to True.\n",
    "\n",
    "You can score between 0 and 100 points for this task. The number of points you earn will be calculated on the (secret) test set on the Competition Platform based on the formula mentioned above, rounded to the nearest integer. If your solution does not meet the above criteria, does not execute correctly, or if an attempt at cheating is detected, you will receive 0 points for the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Code\n",
    "\n",
    "In this section, we initialize the environment by importing the necessary libraries and functions. The prepared code will make it easier for you to operate on data effectively and build the right solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = False  # We will set this flag to True during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "# Additional libraries you can use in your solution\n",
    "import xgboost\n",
    "import sklearn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIR = \"./data\"\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_DIR, \"train.npz\")\n",
    "VALID_DATA_PATH = os.path.join(DATA_DIR, \"valid.npz\")\n",
    "\n",
    "assert torch.cuda.is_available(), \"No graphics card (GPU) found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "seed = 12345\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "Using the code below, we load data containing multispectral images and their corresponding segmentation masks. This data will be the basis for training and validating your segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "# A cell containing auxiliary functions for data preparation.\n",
    "\n",
    "class BaseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multispectral dataset class.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_path: str):\n",
    "        data = np.load(data_path)\n",
    "        self.bands = data[\"bands\"]\n",
    "        self.segmentations = data[\"segmentations\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples in the dataset.\"\"\"\n",
    "        return self.bands.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a sample at index idx - its multispectral bands and segmentation.\"\"\"\n",
    "        bands = self.bands[idx]\n",
    "        segmentations = self.segmentations[idx]\n",
    "\n",
    "        bands = torch.from_numpy(bands).float()\n",
    "        segmentations = torch.from_numpy(segmentations).long()\n",
    "\n",
    "        return bands, segmentations\n",
    "\n",
    "def setup_data():\n",
    "    \"\"\"\n",
    "    Downloads the datasets used in the task.\n",
    "    \"\"\"\n",
    "    import gdown\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(TRAIN_DATA_PATH):\n",
    "        url = \"https://drive.google.com/uc?id=1Nfv3Kd9W8ypjr8RL1jY3VF3yTKU934lz\"\n",
    "        gdown.download(url, TRAIN_DATA_PATH, fuzzy=True)\n",
    "    \n",
    "    if not os.path.exists(VALID_DATA_PATH):\n",
    "        url = \"https://drive.google.com/uc?id=1WUwayYErm4CXI7zHUAmZmi23doIKmDm3\"\n",
    "        gdown.download(url, VALID_DATA_PATH, fuzzy=True)\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    setup_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code with the Evaluation Criterion\n",
    "\n",
    "Code similar to the one below will be used to evaluate the solution on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "def calculate_miou(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the mIoU (mean Intersection over Union) metric used to evaluate the model.\n",
    "    A helper function used in the evaluation of the solution.\n",
    "    \"\"\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    assert y_true.device == y_pred.device\n",
    "\n",
    "    num_classes = 4\n",
    "    device = y_true.device\n",
    "    batch_size = y_true.shape[0]\n",
    "    y_true_flat = y_true.reshape(batch_size, y_true.shape[1] * y_true.shape[2])\n",
    "    y_pred_flat = y_pred.reshape(batch_size, y_pred.shape[1] * y_pred.shape[2])\n",
    "\n",
    "    # The final average is calculated only for classes present in the ground truth data\n",
    "    intersections = torch.zeros((batch_size, num_classes), dtype=torch.float32, device=device)\n",
    "    unions = torch.zeros((batch_size, num_classes), dtype=torch.float32, device=device)\n",
    "    true_present = torch.zeros((batch_size, num_classes), dtype=torch.bool, device=device)\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        y_true_c = (y_true_flat == cls)\n",
    "        y_pred_c = (y_pred_flat == cls)\n",
    "        true_present[:, cls] = torch.any(y_true_c, dim=1)\n",
    "\n",
    "        intersections[:, cls] = torch.sum(y_true_c & y_pred_c, dim=1).to(torch.float32)\n",
    "        unions[:, cls] = torch.sum(y_true_c | y_pred_c, dim=1).to(torch.float32)\n",
    "\n",
    "    # Calculates the number of relevant classes for each sample\n",
    "    num_present_classes = torch.sum(true_present, dim=1).to(torch.float32)\n",
    "\n",
    "    # Although calculations are performed for all classes, we only sum those that are actually present (in the ground truth data)\n",
    "    iou_per_class = torch.nan_to_num(intersections / unions, nan=0.0)\n",
    "    sum_iou = torch.sum(iou_per_class * true_present, dim=1)\n",
    "    miou_scores = torch.nan_to_num(sum_iou / num_present_classes, nan=0.0).tolist()\n",
    "    assert len(miou_scores) == batch_size\n",
    "\n",
    "    return miou_scores\n",
    "\n",
    "\n",
    "def calculate_channel_count(dataloader):\n",
    "    \"\"\"\n",
    "    A helper function used in the evaluation of the solution.\n",
    "    Calculates the number of bands used during model training and evaluation.\n",
    "    \"\"\"\n",
    "    tensors = dataloader.dataset.tensors\n",
    "    \n",
    "    if len(tensors) != 2:\n",
    "        raise ValueError(\"The dataset should only contain labels and images (__getitem__ should return a tuple of dimensionality 2)\")\n",
    "    \n",
    "    x, _ = tensors\n",
    "    \n",
    "    # x.shape [dataset size, channels, height, width]\n",
    "    if x.ndim != 4:\n",
    "        raise ValueError(\"Processed data must have 4 dimensions [batch size, channels, height, width]\")\n",
    "\n",
    "    if x.shape[0] < 15: #the validation set is the smallest and has 15 samples\n",
    "        raise ValueError(f\"The first dimension is reserved for the dataset size, your code should not change it!\")\n",
    "\n",
    "    if x.shape[2] != 30 or x.shape[3] != 30:\n",
    "        raise ValueError(\"Processed data must have dimensions 30x30 on the 3rd and 4th dimension (.shape[2] == .shape[3] == 30)\")\n",
    "\n",
    "    channels_count = x.shape[1]\n",
    "    return channels_count\n",
    "\n",
    "def transform_dataset(preprocessing, dataset:BaseDataset) -> TensorDataset:\n",
    "    \"\"\"\n",
    "    Processes the specified dataset and stores it in the computer's memory (RAM).\n",
    "    Calls the .transform function implemented by the participant in the class that prepares datasets.\n",
    "    \"\"\"\n",
    "    processed_labels = []\n",
    "    processed_images = []\n",
    "    \n",
    "    for raw_image, label in dataset:\n",
    "        processed_labels.append(label.clone())\n",
    "        \n",
    "        transformed_image = preprocessing.transform(raw_image.clone())\n",
    "        processed_images.append(transformed_image)\n",
    "\n",
    "    labels_tensor = torch.stack(processed_labels) \n",
    "    images_tensor = torch.stack(processed_images)  \n",
    "    memory_dataset = TensorDataset(images_tensor, labels_tensor)\n",
    "    return memory_dataset\n",
    "\n",
    "def evaluate(train_model, preprocessing, data_path: str):\n",
    "    \"\"\"\n",
    "    Main function for evaluating the task.\n",
    "    The same function will be called on the Competition Platform.\n",
    "\n",
    "    1. Fits the data preprocessing class on the training set.\n",
    "    2. Processes the specified dataset using the fitted class.\n",
    "    3. Evaluates the model using the mIoU metric on the specified processed dataset.\n",
    "    4. Evaluates the number of bands used for model evaluation - checks the data shape.\n",
    "    \"\"\"\n",
    "\n",
    "    # Loads the training set and fits the data preparation class on it\n",
    "    train_ds = BaseDataset(data_path=TRAIN_DATA_PATH)\n",
    "    preprocessing = preprocessing()\n",
    "    assert hasattr(preprocessing, \"fit\"), \"The data preparation function must implement the .fit method\"\n",
    "    preprocessing.fit(train_ds)\n",
    "    \n",
    "    # Loads the target dataset and processes it using the data preparation class\n",
    "    target_ds = BaseDataset(data_path=data_path)\n",
    "    assert hasattr(preprocessing, \"transform\"), \"The data preparation function must implement the .transform method\"\n",
    "    transformed_dataset = transform_dataset(preprocessing=preprocessing, dataset=target_ds)\n",
    "    dataloader = DataLoader(transformed_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    model = train_model()\n",
    "\n",
    "    if hasattr(model, \"to\") and callable(getattr(model, \"to\", None)):\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "    if hasattr(model, \"eval\") and callable(getattr(model, \"eval\", None)):\n",
    "        model.eval()\n",
    "\n",
    "    mious = []\n",
    "\n",
    "    # Evaluates the model using the mIoU metric\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for x, y in dataloader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            if hasattr(y_pred, \"to\") and callable(getattr(y_pred, \"to\", None)):\n",
    "                y_pred = y_pred.to(DEVICE)\n",
    "            if not torch.is_tensor(y_pred):\n",
    "                y_pred = torch.tensor(y_pred)\n",
    "            y_pred = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "            assert y_pred.shape == y.shape\n",
    "            assert y_pred.max() <= 4 and y_pred.min() >= 0\n",
    "\n",
    "            miou = calculate_miou(y, y_pred)\n",
    "            mious.extend(miou)\n",
    "\n",
    "    # Evaluates the number of bands used for model evaluation\n",
    "    channels_count = calculate_channel_count(dataloader=dataloader)\n",
    "    \n",
    "    # Calculates the average mIoU over all samples\n",
    "    miou = sum(mious) / len(mious)\n",
    "\n",
    "    return miou, channels_count\n",
    "    \n",
    "def compute_score(miou, channels_count):\n",
    "    \"\"\"\n",
    "    Calculates the score for the task, calculates the final number of points for the task using the formula given in the task description.\n",
    "    The same function will be called on the Competition Platform.\n",
    "    \"\"\"\n",
    "    band_score = max(0, min(100, 100 * ((12 - channels_count) / (12 - 3))**2))\n",
    "    miou_score = max(0, min(100, 100 * (miou - 0.6) / (0.8 - 0.6)))\n",
    "\n",
    "    print(f\"Number of channels: {channels_count}\")\n",
    "    print(f\"mIoU: {miou:.3f} \\n\")\n",
    "\n",
    "    print(f\"Score for channels: {band_score:.3f}\")\n",
    "    print(f\"Score for mIoU: {miou_score:.3f}\")\n",
    "\n",
    "    if miou_score < 0.5:\n",
    "        total_score = int(0)\n",
    "    else:\n",
    "        total_score = 0.25 * band_score + 0.75 * miou_score\n",
    "        total_score = int(round(total_score))\n",
    "    print(f\"Estimated number of points for the task: {total_score}\")\n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Solution\n",
    "\n",
    "Below we present a simplified solution based on linear regression, which can serve as an example demonstrating the operation of the notebook as well as a starting point for creating your solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "class BasicPreprocessing():\n",
    "  \"\"\"\n",
    "  Sample implementation of a dataset processing class.\n",
    "\n",
    "  This is just an example solution that does not reduce the number of channels.\n",
    "  The fewer channels you use, the more points you will receive for this part of the assignment.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.std_per_channel = None\n",
    "    \n",
    "  def fit(self, dataset:BaseDataset):\n",
    "\n",
    "    # Zbiera wszystkie obrazy do jednej macierzy [N_próbek, 12, 30, 30]\n",
    "    images = []\n",
    "    for item in dataset:\n",
    "        image_tensor, _ = item\n",
    "        images.append(image_tensor)\n",
    "    images_tensor = torch.stack(images)  # shape: [N, 12, 30, 30]\n",
    "\n",
    "    # Oblicza std (odchylenie standardowe) po wymiarach: (0: próbka, 2: H, 3: W)\n",
    "    self.std_per_channel = images_tensor.std(dim=(0, 2, 3))\n",
    "\n",
    "    return self\n",
    "\n",
    "  def transform(self, image_tensor: torch.Tensor) -> torch.Tensor:          \n",
    "    # Changes the shape of the mean to [12, 1, 1] to allow broadcasting relative to [12, 30, 30].\n",
    "    # Remember, this is just an example solution, simply dividing by std is not the most efficient solution.      std_reshaped = self.std_per_channel.view(-1, 1, 1)\n",
    "     \n",
    "    return image_tensor / std_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(12, 4)\n",
    "    \n",
    "    def forward(self, bands):\n",
    "        # bands.shape [b, c, h, w]\n",
    "        return self.linear(bands.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "def train_basic_model():\n",
    "\n",
    "    epochs = 40\n",
    "    lr = 0.001\n",
    "    batch_size = 8\n",
    "    model = BasicModel()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Prepares data preprocessing parameters only using the\n",
    "    # training set.\n",
    "\n",
    "    raw_train_ds = BaseDataset(data_path=TRAIN_DATA_PATH)\n",
    "    raw_valid_ds = BaseDataset(data_path=VALID_DATA_PATH)\n",
    "\n",
    "    preprocessing = BasicPreprocessing()\n",
    "    preprocessing.fit(raw_train_ds)\n",
    "\n",
    "    train_ds = transform_dataset(preprocessing=preprocessing, dataset=raw_train_ds)\n",
    "    valid_ds = transform_dataset(preprocessing=preprocessing, dataset=raw_valid_ds)\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\"):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = 0\n",
    "            mious = []\n",
    "            for x, y in tqdm(valid_dataloader, total=len(valid_dataloader), desc=\"Validation\"):\n",
    "                x = x.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                y_pred = model(x)\n",
    "                loss = criterion(y_pred, y)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                y_pred = torch.argmax(y_pred, dim=1)\n",
    "                miou = calculate_miou(y, y_pred)\n",
    "                mious.extend(miou)\n",
    "            \n",
    "            valid_loss = valid_loss / len(valid_dataloader)\n",
    "            print(f\"Epoch {epoch+1} loss: {valid_loss}, mIoU: {sum(mious) / len(mious)}\")\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Sample Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    miou, channels_count = evaluate(train_basic_model, BasicPreprocessing, VALID_DATA_PATH)\n",
    "    print(\"-\"*50)\n",
    "    compute_score(miou, channels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Solution\n",
    "Your solution should be in this section. Please make changes here only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not change the class name.\n",
    "# This class can only process samples, it cannot use data labels.\n",
    "# Neither method can perform segmentation.\n",
    "\n",
    "class YourPreprocessing():\n",
    "  def __init__(self):\n",
    "    pass\n",
    "    \n",
    "  def fit(self, dataset: BaseDataset):\n",
    "    return dataset\n",
    "\n",
    "  def transform(self, image_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    return image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, bands):\n",
    "        # random predictions\n",
    "        segmentation = torch.randint(0, 4, (bands.shape[0], 1, 30, 30))\n",
    "        # We perform one hot encoding because evaluation uses torch.argmax\n",
    "        one_hot = torch.zeros(bands.shape[0], 4, 30, 30)\n",
    "        one_hot.scatter_(1, segmentation, 1)\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't change the function name\n",
    "def train_your_model():\n",
    "    return YourModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Running the cell below will allow you to check how many points your solution would score on the validation data. On the Competition Platform, your solution will be evaluated on the test set.\n",
    "\n",
    "Before submitting, make sure that the entire notebook runs from start to finish without errors and without user intervention after executing the `Run All` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    miou, channels_count = evaluate(train_your_model, YourPreprocessing, VALID_DATA_PATH)\n",
    "    print(\"-\"*50)\n",
    "    compute_score(miou, channels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember:** During the evaluation, the model (the model training function) and the data processing function will be evaluated on the test set, not the validation set!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
